# COGS185 Final Project: Text-to-Image Synthesis with Stable Diffusion

## Authors: Tianhao Chen, Yi Zhang

### Date: June 2024

## Abstract
This project explores the application of Stable Diffusion, a state-of-the-art generative model, for image synthesis from textual descriptions. Our objective is to evaluate the model's capability to generate realistic images that align with varying lengths of textual prompts. A dataset comprising photos of specific items was prepared, with descriptions generated by a transformer model. The generated images were evaluated using the Inception Score (IS) and CLIP score to assess quality and relevance. Our findings demonstrate the efficacy of Stable Diffusion in producing high-quality images that accurately reflect the provided textual descriptions, highlighting its potential for various AI-driven content creation and augmentation applications.

## Introduction
The rapid advancement of generative models has enabled the creation of highly realistic images from textual descriptions. Among these models, Stable Diffusion has emerged as a powerful approach, leveraging diffusion processes for image generation. This project investigates the performance of Stable Diffusion in generating images from textual prompts of varying lengths, evaluated using metrics like the Inception Score (IS) and CLIP score.

The ability to generate images from text has implications in domains like entertainment, design, and virtual reality. Our primary focus is to assess how well Stable Diffusion can generate images that accurately represent descriptions of different lengths generated by a transformer.

## Related Work
### Generative Models for Text-to-Image Generation
Generative models like GANs, including StackGAN and AttnGAN, have paved the way for text-to-image synthesis. More recent approaches, such as Denoising Diffusion Probabilistic Models (DDPMs) and Stable Diffusion, achieve state-of-the-art performance by utilizing diffusion processes.

### Pre-trained Language and Vision Models
Models like CLIP (Contrastive Language–Image Pretraining) and BLIP (Bootstrapped Learning from Image-Text Pairs) have proven effective for aligning text and image representations, facilitating cross-modal understanding and evaluation of generated images.

### Evaluation Metrics
We use the Inception Score (IS) for image quality and diversity assessment, and the CLIP Score to measure alignment with textual descriptions.

## Methodology
### Dataset Preparation
We used three distinct datasets (Dog, Cat, and Swan), each containing images and their corresponding textual descriptions. Descriptions were generated using the BLIP model based on Vision Transformers (ViT) from Hugging Face. The dataset was split into training and validation subsets.

### Model Architecture
The Stable Diffusion model from Hugging Face's `runwayml/stable-diffusion-v1-5` was used for image synthesis. Stable Diffusion generates images by refining random noise in iterative denoising steps, guided by textual descriptions.

### Image Generation
For validation, we provided textual descriptions of varying lengths (20-40, 40-60, and 60-80 words) to generate images and assess the model’s generalization capability.

### Implementation Details
The implementation involved:
- Using a transformer to generate text descriptions of prepared images.
- Loading the Stable Diffusion and CLIP models from Hugging Face and transferring them to a GPU.
- Generating images and calculating CLIP scores for evaluation.

## Evaluation Metrics
- **Inception Score (IS)**: Higher scores indicate better quality and diversity in generated images.
- **CLIP Score**: Measures alignment between generated images and textual descriptions.

## Experiment
### Experimental Setup
The experiments were conducted on the Dog, Cat, and Swan datasets, split into training and validation subsets. Text descriptions of varying lengths were used to generate images during the validation phase.

### Model & Training Process
To facilitate reproducibility, the data preparation and model code are publicly available on GitHub:
- **Data Preparation**: [Data Preparation Repository]
- **Model Implementation and Training**: [Model Training Repository]

### Image Generation
During validation, descriptions of different lengths were used to generate images. The images were saved for analysis.

### Quality and Diversity Assessment
We used the Inception Score (IS) for quality and diversity assessment, with average scores as follows:
- **Dog Dataset**: IS = 4.2
- **Cat Dataset**: IS = 4.0
- **Swan Dataset**: IS = 3.8

### Relevance to Textual Descriptions
The CLIP Score was used to assess alignment with descriptions, showing improved alignment with longer captions:
- **Dog Dataset**:
  - 20-40 words: 30.85
  - 40-60 words: 33.56
  - 60-80 words: 34.02
- **Cat Dataset**:
  - 20-40 words: 30.94
  - 40-60 words: 32.90
  - 60-80 words: 33.18
- **Swan Dataset**:
  - 20-40 words: 30.07
  - 40-60 words: 32.06
  - 60-80 words: 32.14

### Visual Results
Generated images closely matched original images for breeds, poses, and activities. However, details in the Swan dataset were less accurately reproduced than in the Dog and Cat datasets.

#### Example Images
Below are some examples of generated images from each dataset:

- **Dog Dataset**
  ![Dog Image Example](images/dog_example.png)

- **Cat Dataset**
  ![Cat Image Example](images/cat_example.png)

- **Swan Dataset**
  ![Swan Image Example](images/swan_example.png)

## Conclusion
This study demonstrated the potential of Stable Diffusion for text-to-image synthesis across three datasets. The high IS and CLIP scores indicate Stable Diffusion’s capability to generate high-quality, relevant images. This work underscores the potential of generative models for content creation in fields like entertainment, design, and virtual reality.

Future work could explore additional techniques to enhance the model's performance and generalization capabilities.

## Repository Contents
- `data_preparation/`: Contains scripts for data preparation and caption generation.
- `model/`: Contains the `model.py` script for image generation and evaluation.
- `images/`: Contains example images for comparison.
- `caption_gen_model.ipynb`: Jupyter notebook for caption generation using the transformer model.
- `dog_model.ipynb`: Jupyter notebook for generating images for the Dog dataset.
- `cat_model.ipynb`: Jupyter notebook for generating images for the Cat dataset.
- `swan_model.ipynb`: Jupyter notebook for generating images for the Swan dataset.

## Getting Started

### Prerequisites
Ensure you have the following software installed:
- Python 3.7+
- PyTorch
- Transformers (Hugging Face)
- Diffusers (Hugging Face)
- torchvision
- pandas
- PIL
- torch-fidelity

### Installation
1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/super-model.git
   cd super-model
   ```

## References
- Reed, S., et al. Generative Adversarial Text to Image Synthesis. arXiv preprint arXiv:1605.05396, 2016.
- Zhang, H., et al. StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks. arXiv preprint arXiv:1612.03242, 2017.
- Xu, T., et al. AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks. arXiv preprint arXiv:1711.10485, 2018.
- Ho, J., et al. Denoising Diffusion Probabilistic Models. arXiv preprint arXiv:2006.11239, 2020.
- Song, Y., et al. Score-Based Generative Modeling through Stochastic Differential Equations. arXiv preprint arXiv:2011.13456, 2020.
- Ramesh, A., et al. Zero-Shot Text-to-Image Generation. arXiv preprint arXiv:2102.12092, 2021.
- Radford, A., et al. Learning Transferable Visual Models From Natural Language Supervision. arXiv preprint arXiv:2103.00020, 2021.
