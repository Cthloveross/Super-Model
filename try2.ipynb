{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24923fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/lixiaozao/opt/anaconda3/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/lixiaozao/opt/anaconda3/lib/python3.9/site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/lixiaozao/opt/anaconda3/lib/python3.9/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/lixiaozao/opt/anaconda3/lib/python3.9/site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in /Users/lixiaozao/opt/anaconda3/lib/python3.9/site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /Users/lixiaozao/opt/anaconda3/lib/python3.9/site-packages (from torch) (3.0.3)\n",
      "Requirement already satisfied: fsspec in /Users/lixiaozao/opt/anaconda3/lib/python3.9/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/lixiaozao/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/lixiaozao/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53e90aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size for cat:  30\n",
      "Test data size for cat:  10\n",
      "Train data size for dog:  30\n",
      "Test data size for dog:  10\n",
      "Train data size for swan:  30\n",
      "Test data size for swan:  10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Paths to datasets\n",
    "paths = {\n",
    "    'cat': '/Users/lixiaozao/Desktop/desk/cogs185/FINAL/Super-Model/dataset/cat/selected_images_captions/captions.csv',\n",
    "    'dog': '/Users/lixiaozao/Desktop/desk/cogs185/FINAL/Super-Model/dataset/dog/selected_images_captions/captions.csv',\n",
    "    'swan': '/Users/lixiaozao/Desktop/desk/cogs185/FINAL/Super-Model/dataset/swan/selected_images_captions/captions.csv'\n",
    "}\n",
    "\n",
    "# Load datasets and split\n",
    "train_data = {}\n",
    "test_data = {}\n",
    "for animal, path in paths.items():\n",
    "    df = pd.read_csv(path)\n",
    "    train, test = train_test_split(df, test_size=10, random_state=42)\n",
    "    train_data[animal] = train.head(30)\n",
    "    test_data[animal] = test\n",
    "\n",
    "# Check the splits\n",
    "for animal in train_data:\n",
    "    print(f\"Train data size for {animal}: \", len(train_data[animal]))\n",
    "    print(f\"Test data size for {animal}: \", len(test_data[animal]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dab480b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lixiaozao/opt/anaconda3/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/lixiaozao/opt/anaconda3/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/lixiaozao/opt/anaconda3/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "class CustomImageCaptionDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.dataframe.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        caption = self.dataframe.iloc[idx, 1]\n",
    "        return image, caption\n",
    "\n",
    "# Define the image directories\n",
    "image_dirs = {\n",
    "    'cat': '/Users/lixiaozao/Desktop/desk/cogs185/FINAL/Super-Model/dataset/cat/selected_images',\n",
    "    'dog': '/Users/lixiaozao/Desktop/desk/cogs185/FINAL/Super-Model/dataset/dog/selected_images',\n",
    "    'swan': '/Users/lixiaozao/Desktop/desk/cogs185/FINAL/Super-Model/dataset/swan/selected_images'\n",
    "}\n",
    "\n",
    "# Combine datasets for DataLoader\n",
    "train_datasets = []\n",
    "for animal, df in train_data.items():\n",
    "    train_datasets.append(CustomImageCaptionDataset(df, image_dirs[animal]))\n",
    "\n",
    "train_dataset = torch.utils.data.ConcatDataset(train_datasets)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb6cc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel, DDPMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 1e-4\n",
    "batch_size = 4\n",
    "num_epochs = 5\n",
    "image_size = 256\n",
    "\n",
    "# Define the image directories\n",
    "image_dirs = {\n",
    "    'cat': 'dataset/cat/selected_images',\n",
    "    'dog': 'dataset/dog/selected_images',\n",
    "    'swan': 'dataset/swan/selected_images'\n",
    "}\n",
    "\n",
    "# Create a combined dataset for training\n",
    "train_datasets = []\n",
    "for animal, df in train_data.items():\n",
    "    train_datasets.append(CustomImageCaptionDataset(df, image_dirs[animal]))\n",
    "train_dataset = torch.utils.data.ConcatDataset(train_datasets)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load pre-trained model\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "# Extract components of the pipeline\n",
    "unet = pipe.unet\n",
    "scheduler = pipe.scheduler\n",
    "vae = pipe.vae\n",
    "text_encoder = pipe.text_encoder\n",
    "tokenizer = pipe.tokenizer\n",
    "\n",
    "# Set models to train mode\n",
    "unet.train()\n",
    "text_encoder.train()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(unet.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        images, captions = batch\n",
    "\n",
    "        # Preprocess captions\n",
    "        inputs = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n",
    "        input_ids = inputs.input_ids.to(\"cuda\")\n",
    "\n",
    "        # Encode the captions\n",
    "        encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "\n",
    "        # Preprocess images\n",
    "        images = [vae.encode(torch.tensor(image).unsqueeze(0).float().to(\"cuda\")) for image in images]\n",
    "        images = torch.cat(images, dim=0)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        noise = torch.randn(images.shape).to(\"cuda\")\n",
    "        timesteps = torch.randint(0, scheduler.num_train_timesteps, (images.shape[0],)).to(\"cuda\")\n",
    "        noisy_images = scheduler.add_noise(images, noise, timesteps)\n",
    "\n",
    "        # Get model prediction\n",
    "        model_pred = unet(noisy_images, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "        # Calculate loss (e.g., MSE loss)\n",
    "        loss = nn.MSELoss()(model_pred, noise)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6efd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Placeholder for training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     for images, captions in train_loader:\n",
    "#         # Preprocess images and captions for the model\n",
    "#         # Forward pass through the model\n",
    "#         # Calculate loss\n",
    "#         # Backward pass and optimization\n",
    "#         pass\n",
    "\n",
    "# # For now, we use the pre-trained model for generation\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
    "# pipe = pipe.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee8ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel, DDPMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 1e-4\n",
    "batch_size = 4\n",
    "num_epochs = 5\n",
    "image_size = 256\n",
    "\n",
    "# Define the image directories\n",
    "image_dirs = {\n",
    "    'cat': 'dataset/cat/selected_images',\n",
    "    'dog': 'dataset/dog/selected_images',\n",
    "    'swan': 'dataset/swan/selected_images'\n",
    "}\n",
    "\n",
    "# Create a combined dataset for training\n",
    "train_datasets = []\n",
    "for animal, df in train_data.items():\n",
    "    train_datasets.append(CustomImageCaptionDataset(df, image_dirs[animal]))\n",
    "train_dataset = torch.utils.data.ConcatDataset(train_datasets)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load pre-trained model\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "# Extract components of the pipeline\n",
    "unet = pipe.unet\n",
    "scheduler = pipe.scheduler\n",
    "vae = pipe.vae\n",
    "text_encoder = pipe.text_encoder\n",
    "tokenizer = pipe.tokenizer\n",
    "\n",
    "# Set models to train mode\n",
    "unet.train()\n",
    "text_encoder.train()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(unet.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        images, captions = batch\n",
    "\n",
    "        # Preprocess captions\n",
    "        inputs = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n",
    "        input_ids = inputs.input_ids.to(\"cuda\")\n",
    "\n",
    "        # Encode the captions\n",
    "        encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "\n",
    "        # Preprocess images\n",
    "        images = [vae.encode(torch.tensor(image).unsqueeze(0).float().to(\"cuda\")) for image in images]\n",
    "        images = torch.cat(images, dim=0)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        noise = torch.randn(images.shape).to(\"cuda\")\n",
    "        timesteps = torch.randint(0, scheduler.num_train_timesteps, (images.shape[0],)).to(\"cuda\")\n",
    "        noisy_images = scheduler.add_noise(images, noise, timesteps)\n",
    "\n",
    "        # Get model prediction\n",
    "        model_pred = unet(noisy_images, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "        # Calculate loss (e.g., MSE loss)\n",
    "        loss = nn.MSELoss()(model_pred, noise)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10b1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate images using the Stable Diffusion pipeline\n",
    "def generate_image(prompt, model):\n",
    "    image = model(prompt).images[0]\n",
    "    return image\n",
    "\n",
    "# Generate images for test captions\n",
    "for animal, df in test_data.items():\n",
    "    df['generated_image'] = df['caption'].apply(lambda x: generate_image(x, pipe))\n",
    "\n",
    "# Save generated images\n",
    "\n",
    "import os\n",
    "\n",
    "output_dir = 'Super-Model/dataset/gen_imag'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for animal, df in test_data.items():\n",
    "    for i, row in df.iterrows():\n",
    "        image_path = os.path.join(output_dir, f\"{animal}_generated_{i}.png\")\n",
    "        row['generated_image'].save(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72405fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def calculate_clip_score(image, text, model, processor):\n",
    "    inputs = processor(text=[text], images=image, return_tensors=\"pt\", padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    return logits_per_image.item()\n",
    "\n",
    "# Calculate CLIP scores for the test set\n",
    "for animal, df in test_data.items():\n",
    "    df['clip_score'] = df.apply(lambda x: calculate_clip_score(x['generated_image'], x['caption'], clip_model, clip_processor), axis=1)\n",
    "\n",
    "# Output the CLIP scores\n",
    "for animal, df in test_data.items():\n",
    "    print(f\"CLIP scores for {animal}:\")\n",
    "    print(df[['caption', 'clip_score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23691c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
