{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1199d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (10.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53e90aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size for cat:  30\n",
      "Test data size for cat:  10\n",
      "Train data size for dog:  30\n",
      "Test data size for dog:  10\n",
      "Train data size for swan:  30\n",
      "Test data size for swan:  10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Paths to datasets\n",
    "paths = {\n",
    "    'cat': 'dataset/cat/selected_images_captions/captions.csv',\n",
    "    'dog': 'dataset/dog/selected_images_captions/captions.csv',\n",
    "    'swan': 'dataset/swan/selected_images_captions/captions.csv'\n",
    "}\n",
    "\n",
    "# Load datasets and split\n",
    "train_data = {}\n",
    "test_data = {}\n",
    "for animal, path in paths.items():\n",
    "    df = pd.read_csv(path)\n",
    "    train, test = train_test_split(df, test_size=10, random_state=42)\n",
    "    train_data[animal] = train.head(30)\n",
    "    test_data[animal] = test\n",
    "\n",
    "# Check the splits\n",
    "for animal in train_data:\n",
    "    print(f\"Train data size for {animal}: \", len(train_data[animal]))\n",
    "    print(f\"Test data size for {animal}: \", len(test_data[animal]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "349ac249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from diffusers import StableDiffusionPipeline, UNet2DConditionModel, DDPMScheduler\n",
    "# from transformers import CLIPTextModel, CLIPTokenizer\n",
    "# import torch\n",
    "# from torch import nn, optim\n",
    "# from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "# import os\n",
    "\n",
    "# # Define hyperparameters\n",
    "# learning_rate = 1e-4\n",
    "# batch_size = 1\n",
    "# num_epochs = 5\n",
    "# image_size = 256\n",
    "\n",
    "# # Define transformations\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),  # Resize images to a standard size\n",
    "#     transforms.ToTensor()  # Convert PIL Image to tensor\n",
    "# ])\n",
    "\n",
    "# class CustomImageCaptionDataset(Dataset):\n",
    "#     def __init__(self, dataframe, image_dir, transform=None):\n",
    "#         self.dataframe = dataframe\n",
    "#         self.image_dir = image_dir\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataframe)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_name = os.path.join(self.image_dir, self.dataframe.iloc[idx, 0])\n",
    "#         image = Image.open(img_name).convert(\"RGB\")\n",
    "#         caption = self.dataframe.iloc[idx, 1]\n",
    "\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "\n",
    "#         return image, caption\n",
    "\n",
    "# # Define the image directories\n",
    "# image_dirs = {\n",
    "#     'cat': 'dataset/cat/selected_images',\n",
    "#     'dog': 'dataset/dog/selected_images',\n",
    "#     'swan': 'dataset/swan/selected_images'\n",
    "# }\n",
    "\n",
    "# # Assuming train_data is a dictionary where keys are 'cat', 'dog', 'swan'\n",
    "# # and values are corresponding dataframes.\n",
    "# train_datasets = []\n",
    "# for animal, df in train_data.items():\n",
    "#     train_datasets.append(CustomImageCaptionDataset(df, image_dirs[animal], transform=transform))\n",
    "\n",
    "# train_dataset = ConcatDataset(train_datasets)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# # Load pre-trained model\n",
    "# model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "# pipe = pipe.to(\"cuda\")\n",
    "\n",
    "# # Extract components of the pipeline\n",
    "# unet = pipe.unet\n",
    "# scheduler = pipe.scheduler\n",
    "# vae = pipe.vae\n",
    "# text_encoder = pipe.text_encoder\n",
    "# tokenizer = pipe.tokenizer\n",
    "\n",
    "# # Set models to train mode\n",
    "# unet.train()\n",
    "# text_encoder.train()\n",
    "\n",
    "# # Define optimizer\n",
    "# optimizer = optim.Adam(unet.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in train_loader:\n",
    "#         images, captions = batch\n",
    "\n",
    "#         # Preprocess captions\n",
    "#         inputs = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n",
    "#         input_ids = inputs.input_ids.to(\"cuda\")\n",
    "\n",
    "#         # Encode the captions\n",
    "#         encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "\n",
    "#         # Preprocess images\n",
    "#         encoded_images = [vae.encode(image.unsqueeze(0).to(\"cuda\").half()).latent_dist for image in images]\n",
    "#         images = torch.cat(encoded_images, dim=0)\n",
    "\n",
    "#         # Forward pass through the model\n",
    "#         noise = torch.randn(images.shape).to(\"cuda\").half()\n",
    "#         timesteps = torch.randint(0, scheduler.num_train_timesteps, (images.shape[0],)).to(\"cuda\")\n",
    "#         noisy_images = scheduler.add_noise(images, noise, timesteps)\n",
    "\n",
    "#         # Get model prediction\n",
    "#         model_pred = unet(noisy_images, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "#         # Calculate loss (e.g., MSE loss)\n",
    "#         loss = nn.MSELoss()(model_pred, noise)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "787850de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.31.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\77099\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\77099\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (2.0.1+cu117)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (0.23.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.15.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub->accelerate) (2024.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\77099\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dab480b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# class CustomImageCaptionDataset(Dataset):\n",
    "#     def __init__(self, dataframe, image_dir):\n",
    "#         self.dataframe = dataframe\n",
    "#         self.image_dir = image_dir\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataframe)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_name = os.path.join(self.image_dir, self.dataframe.iloc[idx, 0])\n",
    "#         image = Image.open(img_name).convert(\"RGB\")\n",
    "#         caption = self.dataframe.iloc[idx, 1]\n",
    "#         return image, caption\n",
    "\n",
    "# # Define the image directories\n",
    "# image_dirs = {\n",
    "#     'cat': 'dataset/cat/selected_images',\n",
    "#     'dog': 'dataset/dog/selected_images',\n",
    "#     'swan': 'dataset/swan/selected_images'\n",
    "# }\n",
    "\n",
    "# # Combine datasets for DataLoader\n",
    "# train_datasets = []\n",
    "# for animal, df in train_data.items():\n",
    "#     train_datasets.append(CustomImageCaptionDataset(df, image_dirs[animal]))\n",
    "\n",
    "# train_dataset = torch.utils.data.ConcatDataset(train_datasets)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab5e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# import os\n",
    "\n",
    "# # Define transformations\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),  # Resize images to a standard size\n",
    "#     transforms.ToTensor()  # Convert PIL Image to tensor\n",
    "# ])\n",
    "\n",
    "# class CustomImageCaptionDataset(Dataset):\n",
    "#     def __init__(self, dataframe, image_dir, transform=None):\n",
    "#         self.dataframe = dataframe\n",
    "#         self.image_dir = image_dir\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataframe)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_name = os.path.join(self.image_dir, self.dataframe.iloc[idx, 0])\n",
    "#         image = Image.open(img_name).convert(\"RGB\")\n",
    "#         caption = self.dataframe.iloc[idx, 1]\n",
    "\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "\n",
    "#         return image, caption\n",
    "\n",
    "# # Define the image directories\n",
    "# image_dirs = {\n",
    "#     'cat': 'dataset/cat/selected_images',\n",
    "#     'dog': 'dataset/dog/selected_images',\n",
    "#     'swan': 'dataset/swan/selected_images'\n",
    "# }\n",
    "\n",
    "# # Combine datasets for DataLoader\n",
    "# train_datasets = []\n",
    "# for animal, df in train_data.items():\n",
    "#     train_datasets.append(CustomImageCaptionDataset(df, image_dirs[animal], transform=transform))\n",
    "\n",
    "# train_dataset = ConcatDataset(train_datasets)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccb6cc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from diffusers import StableDiffusionPipeline, UNet2DConditionModel, DDPMScheduler\n",
    "# from transformers import CLIPTextModel, CLIPTokenizer\n",
    "# import torch\n",
    "# from torch import nn, optim\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "\n",
    "# # Define hyperparameters\n",
    "# learning_rate = 1e-4\n",
    "# batch_size = 4\n",
    "# num_epochs = 5\n",
    "# image_size = 256\n",
    "\n",
    "# # # Define the image directories\n",
    "# # image_dirs = {\n",
    "# #     'cat': 'dataset/cat/selected_images',\n",
    "# #     'dog': 'dataset/dog/selected_images',\n",
    "# #     'swan': 'dataset/swan/selected_images'\n",
    "# # }\n",
    "\n",
    "# # # Create a combined dataset for training\n",
    "# # train_datasets = []\n",
    "# # for animal, df in train_data.items():\n",
    "# #     train_datasets.append(CustomImageCaptionDataset(df, image_dirs[animal]))\n",
    "# # train_dataset = torch.utils.data.ConcatDataset(train_datasets)\n",
    "# # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Load pre-trained model\n",
    "# model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "# pipe = pipe.to(\"cuda\")\n",
    "\n",
    "# # Extract components of the pipeline\n",
    "# unet = pipe.unet\n",
    "# scheduler = pipe.scheduler\n",
    "# vae = pipe.vae\n",
    "# text_encoder = pipe.text_encoder\n",
    "# tokenizer = pipe.tokenizer\n",
    "\n",
    "# # Set models to train mode\n",
    "# unet.train()\n",
    "# text_encoder.train()\n",
    "\n",
    "# # Define optimizer\n",
    "# optimizer = optim.Adam(unet.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in train_loader:\n",
    "#         images, captions = batch\n",
    "\n",
    "#         # Preprocess captions\n",
    "#         inputs = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n",
    "#         input_ids = inputs.input_ids.to(\"cuda\")\n",
    "\n",
    "#         # Encode the captions\n",
    "#         encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "\n",
    "#         # Preprocess images\n",
    "#         images = [vae.encode(torch.tensor(image).unsqueeze(0).float().to(\"cuda\")) for image in images]\n",
    "#         images = torch.cat(images, dim=0)\n",
    "\n",
    "#         # Forward pass through the model\n",
    "#         noise = torch.randn(images.shape).to(\"cuda\")\n",
    "#         timesteps = torch.randint(0, scheduler.num_train_timesteps, (images.shape[0],)).to(\"cuda\")\n",
    "#         noisy_images = scheduler.add_noise(images, noise, timesteps)\n",
    "\n",
    "#         # Get model prediction\n",
    "#         model_pred = unet(noisy_images, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "#         # Calculate loss (e.g., MSE loss)\n",
    "#         loss = nn.MSELoss()(model_pred, noise)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc6efd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Placeholder for training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     for images, captions in train_loader:\n",
    "#         # Preprocess images and captions for the model\n",
    "#         # Forward pass through the model\n",
    "#         # Calculate loss\n",
    "#         # Backward pass and optimization\n",
    "#         pass\n",
    "\n",
    "# # For now, we use the pre-trained model for generation\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
    "# pipe = pipe.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e3bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fee8ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:07<00:00,  1.04s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 6.00 GiB total capacity; 5.25 GiB already allocated; 0 bytes free; 5.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\77099\\Desktop\\Super-Model\\try2.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/77099/Desktop/Super-Model/try2.ipynb#W5sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39m# Preprocess images\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/77099/Desktop/Super-Model/try2.ipynb#W5sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m \u001b[39mwith\u001b[39;00m autocast():\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/77099/Desktop/Super-Model/try2.ipynb#W5sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m     encoded_images \u001b[39m=\u001b[39m [vae\u001b[39m.\u001b[39;49mencode(image\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mhalf())\u001b[39m.\u001b[39;49mlatent_dist\u001b[39m.\u001b[39;49msample() \u001b[39mfor\u001b[39;49;00m image \u001b[39min\u001b[39;49;00m images]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/77099/Desktop/Super-Model/try2.ipynb#W5sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m     images \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(encoded_images, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/77099/Desktop/Super-Model/try2.ipynb#W5sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     \u001b[39m# Forward pass through the model with checkpointing\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\77099\\Desktop\\Super-Model\\try2.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/77099/Desktop/Super-Model/try2.ipynb#W5sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39m# Preprocess images\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/77099/Desktop/Super-Model/try2.ipynb#W5sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m \u001b[39mwith\u001b[39;00m autocast():\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/77099/Desktop/Super-Model/try2.ipynb#W5sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m     encoded_images \u001b[39m=\u001b[39m [vae\u001b[39m.\u001b[39;49mencode(image\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mhalf())\u001b[39m.\u001b[39mlatent_dist\u001b[39m.\u001b[39msample() \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/77099/Desktop/Super-Model/try2.ipynb#W5sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m     images \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(encoded_images, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/77099/Desktop/Super-Model/try2.ipynb#W5sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     \u001b[39m# Forward pass through the model with checkpointing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\77099\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\diffusers\\utils\\accelerate_utils.py:46\u001b[0m, in \u001b[0;36mapply_forward_hook.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_hf_hook\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hf_hook, \u001b[39m\"\u001b[39m\u001b[39mpre_forward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     45\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpre_forward(\u001b[39mself\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\77099\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\diffusers\\models\\autoencoders\\autoencoder_kl.py:264\u001b[0m, in \u001b[0;36mAutoencoderKL.encode\u001b[1;34m(self, x, return_dict)\u001b[0m\n\u001b[0;32m    262\u001b[0m     h \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(encoded_slices)\n\u001b[0;32m    263\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 264\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[0;32m    266\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquant_conv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    267\u001b[0m     moments \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquant_conv(h)\n",
      "File \u001b[1;32mc:\\Users\\77099\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\77099\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\diffusers\\models\\autoencoders\\vae.py:172\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, sample)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[39m# down\u001b[39;00m\n\u001b[0;32m    171\u001b[0m     \u001b[39mfor\u001b[39;00m down_block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_blocks:\n\u001b[1;32m--> 172\u001b[0m         sample \u001b[39m=\u001b[39m down_block(sample)\n\u001b[0;32m    174\u001b[0m     \u001b[39m# middle\u001b[39;00m\n\u001b[0;32m    175\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmid_block(sample)\n",
      "File \u001b[1;32mc:\\Users\\77099\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\77099\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\diffusers\\models\\unets\\unet_2d_blocks.py:1474\u001b[0m, in \u001b[0;36mDownEncoderBlock2D.forward\u001b[1;34m(self, hidden_states, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1471\u001b[0m     deprecate(\u001b[39m\"\u001b[39m\u001b[39mscale\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m1.0.0\u001b[39m\u001b[39m\"\u001b[39m, deprecation_message)\n\u001b[0;32m   1473\u001b[0m \u001b[39mfor\u001b[39;00m resnet \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresnets:\n\u001b[1;32m-> 1474\u001b[0m     hidden_states \u001b[39m=\u001b[39m resnet(hidden_states, temb\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m   1476\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsamplers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1477\u001b[0m     \u001b[39mfor\u001b[39;00m downsampler \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsamplers:\n",
      "File \u001b[1;32mc:\\Users\\77099\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\77099\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\diffusers\\models\\resnet.py:351\u001b[0m, in \u001b[0;36mResnetBlock2D.forward\u001b[1;34m(self, input_tensor, temb, *args, **kwargs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[39mif\u001b[39;00m temb \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    350\u001b[0m         hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m temb\n\u001b[1;32m--> 351\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm2(hidden_states)\n\u001b[0;32m    352\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_embedding_norm \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mscale_shift\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    353\u001b[0m     \u001b[39mif\u001b[39;00m temb \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\77099\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\77099\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:273\u001b[0m, in \u001b[0;36mGroupNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 273\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mgroup_norm(\n\u001b[0;32m    274\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_groups, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
      "File \u001b[1;32mc:\\Users\\77099\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:2530\u001b[0m, in \u001b[0;36mgroup_norm\u001b[1;34m(input, num_groups, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2528\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected at least 2 dimensions for input tensor but received \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2529\u001b[0m _verify_batch_size([\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m*\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m num_groups, num_groups] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()[\u001b[39m2\u001b[39m:]))\n\u001b[1;32m-> 2530\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mgroup_norm(\u001b[39minput\u001b[39;49m, num_groups, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 6.00 GiB total capacity; 5.25 GiB already allocated; 0 bytes free; 5.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel, DDPMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 1e-4\n",
    "batch_size = 2  # Reduced batch size\n",
    "num_epochs = 5\n",
    "image_size = 256\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a standard size\n",
    "    transforms.ToTensor()  # Convert PIL Image to tensor\n",
    "])\n",
    "\n",
    "class CustomImageCaptionDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.dataframe.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        caption = self.dataframe.iloc[idx, 1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, caption\n",
    "\n",
    "# Define the image directories\n",
    "image_dirs = {\n",
    "    'cat': 'dataset/cat/selected_images',\n",
    "    'dog': 'dataset/dog/selected_images',\n",
    "    'swan': 'dataset/swan/selected_images'\n",
    "}\n",
    "\n",
    "# Assuming train_data is a dictionary where keys are 'cat', 'dog', 'swan'\n",
    "# and values are corresponding dataframes.\n",
    "train_datasets = []\n",
    "for animal, df in train_data.items():\n",
    "    train_datasets.append(CustomImageCaptionDataset(df, image_dirs[animal], transform=transform))\n",
    "\n",
    "train_dataset = ConcatDataset(train_datasets)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load components individually and move them to GPU\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "\n",
    "vae = pipe.vae\n",
    "vae.to(\"cuda\")\n",
    "\n",
    "unet = pipe.unet\n",
    "unet.to(\"cuda\")\n",
    "\n",
    "scheduler = pipe.scheduler\n",
    "\n",
    "text_encoder = pipe.text_encoder\n",
    "text_encoder.to(\"cuda\")\n",
    "\n",
    "tokenizer = pipe.tokenizer\n",
    "\n",
    "# Set models to train mode\n",
    "unet.train()\n",
    "text_encoder.train()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(unet.parameters(), lr=learning_rate)\n",
    "\n",
    "# Mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Custom function for checkpointing\n",
    "def custom_forward(*inputs):\n",
    "    return unet(*inputs)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        images, captions = batch\n",
    "\n",
    "        # Preprocess captions\n",
    "        inputs = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n",
    "        input_ids = inputs.input_ids.to(\"cuda\")\n",
    "\n",
    "        # Encode the captions\n",
    "        encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "\n",
    "        # Preprocess images\n",
    "        with autocast():\n",
    "            encoded_images = [vae.encode(image.unsqueeze(0).to(\"cuda\").half()).latent_dist.sample() for image in images]\n",
    "            images = torch.cat(encoded_images, dim=0)\n",
    "\n",
    "            # Forward pass through the model with checkpointing\n",
    "            noise = torch.randn(images.shape, device=images.device).half()\n",
    "            timesteps = torch.randint(0, scheduler.num_train_timesteps, (images.shape[0],), device=images.device)\n",
    "            noisy_images = scheduler.add_noise(images, noise, timesteps)\n",
    "\n",
    "            # Use torch.utils.checkpoint to save memory\n",
    "            model_pred = checkpoint.checkpoint(custom_forward, noisy_images, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "            # Calculate loss (e.g., MSE loss)\n",
    "            loss = nn.MSELoss()(model_pred, noise)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Clear CUDA cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e522602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.31.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\77099\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\77099\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (2.0.1+cu117)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (0.23.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.15.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub->accelerate) (2024.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\77099\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\77099\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10b1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate images using the Stable Diffusion pipeline\n",
    "def generate_image(prompt, model):\n",
    "    image = model(prompt).images[0]\n",
    "    return image\n",
    "\n",
    "# Generate images for test captions\n",
    "for animal, df in test_data.items():\n",
    "    df['generated_image'] = df['caption'].apply(lambda x: generate_image(x, pipe))\n",
    "\n",
    "# Save generated images\n",
    "\n",
    "import os\n",
    "\n",
    "output_dir = 'Super-Model/dataset/gen_imag'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for animal, df in test_data.items():\n",
    "    for i, row in df.iterrows():\n",
    "        image_path = os.path.join(output_dir, f\"{animal}_generated_{i}.png\")\n",
    "        row['generated_image'].save(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72405fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def calculate_clip_score(image, text, model, processor):\n",
    "    inputs = processor(text=[text], images=image, return_tensors=\"pt\", padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    return logits_per_image.item()\n",
    "\n",
    "# Calculate CLIP scores for the test set\n",
    "for animal, df in test_data.items():\n",
    "    df['clip_score'] = df.apply(lambda x: calculate_clip_score(x['generated_image'], x['caption'], clip_model, clip_processor), axis=1)\n",
    "\n",
    "# Output the CLIP scores\n",
    "for animal, df in test_data.items():\n",
    "    print(f\"CLIP scores for {animal}:\")\n",
    "    print(df[['caption', 'clip_score']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23691c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
