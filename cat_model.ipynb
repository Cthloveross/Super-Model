{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe05440f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 07:09:15.953467: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/tic001/.local/lib/python3.9/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f59baa838a4645b6de9b8e1a46d5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tic001/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: captions_20to40.csv, saving to: cat_gener_image_20to40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7beb5a326324db5bfa6588d204fe85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generated image: cat_gener_image_20to40/15_cat.jpg\n",
      "CLIP score for 15_cat.jpg: 29.760818481445312\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1599153478d642999782d76459611b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generated image: cat_gener_image_20to40/13_cat.jpg\n",
      "CLIP score for 13_cat.jpg: 31.52541160583496\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a87289ff854e7b8596a036bd0b4272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generated image: cat_gener_image_20to40/8_cat.jpg\n",
      "CLIP score for 8_cat.jpg: 31.817960739135742\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1da4603258845e2819ed477f679b96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generated image: cat_gener_image_20to40/14_cat.jpg\n",
      "CLIP score for 14_cat.jpg: 27.86254119873047\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b45b3a0b15c4a03904c5e9924fb3db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generated image: cat_gener_image_20to40/20_cat.jpg\n",
      "CLIP score for 20_cat.jpg: 29.704147338867188\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455855904ce74004a01c0f097a4eca8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generated image: cat_gener_image_20to40/11_cat.jpg\n",
      "CLIP score for 11_cat.jpg: 30.879558563232422\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2066bf7a4543299e8ff5bddcd63ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generated image: cat_gener_image_20to40/18_cat.jpg\n",
      "CLIP score for 18_cat.jpg: 29.525375366210938\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0930469787404c208a54b842dc947820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generated image: cat_gener_image_20to40/2_cat.jpg\n",
      "CLIP score for 2_cat.jpg: 33.00474548339844\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2c57bdc3a745de8bb55b9faa9e10bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generated image: cat_gener_image_20to40/12_cat.jpg\n",
      "CLIP score for 12_cat.jpg: 35.862117767333984\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b78b949e8c4319ae569582b130118e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generated image: cat_gener_image_20to40/16_cat.jpg\n",
      "CLIP score for 16_cat.jpg: 29.44328498840332\n",
      "Average CLIP score for captions_20to40.csv: 30.938596153259276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['logits_unbiased']\n",
      "Extracting features from input1\n",
      "Looking for samples non-recursivelty in \"temp_images\" with extensions png,jpg,jpeg\n",
      "Found 10 samples\n",
      "Processing samples                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception Score: 1.0 ± 7.850462293418876e-17\n",
      "Saved image info to: cat_gener_image_20to40/image_info.csv\n",
      "Average CLIP score for cat_gener_image_20to40: 30.938596153259276\n",
      "Processing dataset: captions_40to60.csv, saving to: cat_gener_image_40to60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inception Score: 1.0 ± 7.850462293418876e-17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a3ebca0bc28400bbfb387b68a080bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generated image: cat_gener_image_40to60/5_cat.jpg\n",
      "CLIP score for 5_cat.jpg: 32.93206024169922\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86b0b350d0c42b1a03ab6bd3f69a70d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generated image: cat_gener_image_40to60/20_cat.jpg\n",
      "CLIP score for 20_cat.jpg: 30.60572052001953\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20152b754bea44b58c777ecc14245523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generated image: cat_gener_image_40to60/17_cat.jpg\n",
      "CLIP score for 17_cat.jpg: 33.797950744628906\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09eb3fe5e4ea4da087d51b0f5a198d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torch_fidelity import calculate_metrics\n",
    "import shutil\n",
    "\n",
    "# Disable tokenizers parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Load the Stable Diffusion pipeline\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "# Load the CLIP model and processor\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Define the paths to your datasets and the output directory\n",
    "dataset_files = [\"captions_20to40.csv\",\"captions_40to60.csv\", \"captions_60to80.csv\"]\n",
    "output_dirs = [\"cat_gener_image_20to40\", \"cat_gener_image_40to60\", \"cat_gener_image_60to80\"]\n",
    "\n",
    "# Define a function to calculate CLIP score\n",
    "def calculate_clip_score(image, text):\n",
    "    inputs = clip_processor(text=[text], images=image, return_tensors=\"pt\")\n",
    "    outputs = clip_model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    return logits_per_image.item()\n",
    "\n",
    "# Iterate over each dataset file\n",
    "for dataset_path, output_dir in zip(dataset_files, output_dirs):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Processing dataset: {dataset_path}, saving to: {output_dir}\")\n",
    "\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(dataset_path)\n",
    "\n",
    "    # Initialize lists for storing images and captions for Inception Score calculation\n",
    "    generated_images = []\n",
    "\n",
    "    # List to store image info\n",
    "    image_info = []\n",
    "\n",
    "    # List to store CLIP scores\n",
    "    clip_scores = []\n",
    "\n",
    "    # Iterate over each row in the CSV file\n",
    "    for index, row in data.iterrows():\n",
    "        image_name = row['image']\n",
    "        caption = row['caption']\n",
    "\n",
    "        # Generate the image using the caption\n",
    "        generated_image = pipe(caption).images[0]\n",
    "        generated_images.append(generated_image)\n",
    "\n",
    "        # Create a unique output file name\n",
    "        output_file_name = image_name\n",
    "        output_path = os.path.join(output_dir, output_file_name)\n",
    "        \n",
    "        # Save the generated image to the output directory\n",
    "        generated_image.save(output_path)\n",
    "        print(f\"Saved generated image: {output_path}\")\n",
    "\n",
    "        # Calculate and print the CLIP score\n",
    "        clip_score = calculate_clip_score(generated_image, caption)\n",
    "        print(f\"CLIP score for {output_file_name}: {clip_score}\")\n",
    "\n",
    "        # Store the CLIP score\n",
    "        clip_scores.append(clip_score)\n",
    "\n",
    "        # Store the information in the list\n",
    "        image_info.append({\n",
    "            'image_name': output_file_name,\n",
    "            'caption': caption,\n",
    "            'clip_score': clip_score\n",
    "        })\n",
    "\n",
    "    # Calculate the average CLIP score\n",
    "    average_clip_score = np.mean(clip_scores)\n",
    "    print(f\"Average CLIP score for {dataset_path}: {average_clip_score}\")\n",
    "\n",
    "    # Preprocess images for Inception Score calculation\n",
    "    preprocessed_images = [transforms.ToTensor()(img).unsqueeze(0) for img in generated_images]\n",
    "    preprocessed_images = torch.cat(preprocessed_images, dim=0).to(\"cuda\")\n",
    "\n",
    "    # Save preprocessed images to a temporary directory\n",
    "    temp_dir = \"temp_images\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    for i, img in enumerate(preprocessed_images):\n",
    "        img_pil = transforms.ToPILImage()(img)\n",
    "        img_pil.save(os.path.join(temp_dir, f\"img_{i}.png\"))\n",
    "\n",
    "    # Calculate Inception Score using torch-fidelity\n",
    "    metrics = calculate_metrics(input1=temp_dir, input1_model=\"inception-v3\", isc=True)\n",
    "    inception_score = metrics[\"inception_score_mean\"]\n",
    "    inception_score_std = metrics[\"inception_score_std\"]\n",
    "    print(f\"Inception Score: {inception_score} ± {inception_score_std}\")\n",
    "\n",
    "    # Clean up temporary directory\n",
    "    shutil.rmtree(temp_dir)\n",
    "\n",
    "    # Add Inception Score to the image info list\n",
    "    image_info.append({\n",
    "        'inception_score': inception_score,\n",
    "        'inception_score_std': inception_score_std\n",
    "    })\n",
    "\n",
    "    # Save the image info to a CSV file\n",
    "    image_info_df = pd.DataFrame(image_info)\n",
    "    image_info_csv_path = os.path.join(output_dir, 'image_info.csv')\n",
    "    image_info_df.to_csv(image_info_csv_path, index=False)\n",
    "    print(f\"Saved image info to: {image_info_csv_path}\")\n",
    "\n",
    "    # Print the average CLIP score\n",
    "    print(f\"Average CLIP score for {output_dir}: {average_clip_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bf03a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
